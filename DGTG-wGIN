from torch.utils.data import Dataset,DataLoader
import numpy as np
import torch.nn as nn
import torch.optim as optim
import torch
from torch_geometric.nn import MessagePassing
import torch_geometric.nn as gnn

class DynamicGIN(MessagePassing):
    def __init__(self, linearLayer):
        super().__init__(aggr='add', flow='source_to_target')
        self.linear = linearLayer

    def forward(self, x, edgeIndex, edgeWeight):
        result = self.propagate(edgeIndex, x=x, edgeWeight=edgeWeight)
        result = self.linear(result)
        return result

    def message(self, x_j, edgeWeight):
        return edgeWeight * x_j

class DGTGWGIN(nn.Module):
    def __init__(self, nodeAmount, dropoutRate=0.2):
        super().__init__()
        self.nodeAmount = nodeAmount  
        self.dropout = dropoutRate

        self.cnnPart = nn.Sequential(
            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(7, 1), stride=(4, 1), padding=0),  
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=(3, 1), stride=(3, 1), padding=0),
            nn.Dropout(self.dropout),

            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(5, 1), stride=(3, 1), padding=0), 
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=(3, 1), stride=(3, 1), padding=0),  
            nn.Dropout(self.dropout),

            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(4, 8), stride=(1, 8), padding=0),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.Dropout(self.dropout),
        )

        
        self.dynamicTopology = dynamicTopology(self.nodeAmount,self.dropout)

        
        self.mlp1 = nn.Sequential(
            nn.Linear(128, 128),
            nn.ReLU(),
        )
        self.mlp2 = nn.Sequential(
            nn.Linear(128, 65),
            nn.ReLU(),
        )
        self.dynamicGIN = DynamicGIN(linearLayer=self.mlp1)
        self.dynamicGIN2 = DynamicGIN(linearLayer=self.mlp2)
        self.readOut = nn.Linear(65, 65)

    def forward(self, x):
        batchSize = x.shape[0]
        out = self.cnnPart(x)  # batchSize * 256 * 13 * 1
        
        outForcoe = out.clone()
        outForcoe = outForcoe.detach()

        coe = self.dynamicTopology(outForcoe)

        
        nodeVector = out.squeeze(3)  # batchSize*channel*nodeAmount
        nodeVector = torch.transpose(nodeVector, 1, 2).contiguous()
        nodeVector = nodeVector.view(batchSize * self.nodeAmount, -1)
        
        edgeIndex = self.generateEdges(batchSize,self.nodeAmount).to(device=x.device)

        #edgeWeight = self.edgeWeight.unsqueeze(0).repeat(batchSize,1,1)
        edgeWeight = nn.ReLU()(nn.Sigmoid()(coe.view(-1, 1)) - 0.5) * 2
        self.edgeWeight = edgeWeight.view(batchSize,self.nodeAmount,self.nodeAmount)
        
        result = self.dynamicGIN(nodeVector, edgeIndex, edgeWeight)
        result = self.dynamicGIN2(result, edgeIndex, edgeWeight)
        result = result.view(batchSize, self.nodeAmount, -1)
        result = torch.sum(result, dim=1)
        return self.readOut(result)

    
    def generateEdges(self, batchSize, nodeAmount):
        start = []
        end = []
        for top in range(batchSize):
            for k in range(nodeAmount):
                start += [k + top * nodeAmount] * (nodeAmount)
                end += [i + top * nodeAmount for i in range(nodeAmount)]
        edges = [start, end]
        return torch.tensor(edges, dtype=torch.long)

class dynamicTopology(nn.Module):
    def __init__(self, nodeAmount,dropout):
        super().__init__()
        self.nodeAmount = nodeAmount
        self.dropout = dropout
        self.coefficientPart = nn.Sequential(

            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(1, 7), stride=(1, 3)),  # 4
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.Dropout(self.dropout),
            nn.MaxPool2d(kernel_size=(1, 4), stride=1, padding=0),  # 1

        )
        self.linear = nn.Linear(256, nodeAmount * nodeAmount)

    def forward(self, x):
        x = self.coefficientPart(x) #batchSize * 1 * 16
        x = x.view(x.shape[0], -1)
        x = self.linear(x)
        return x
